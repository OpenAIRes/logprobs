# logprobs

This repository contains a raw `logprobs.json` capture from the OpenAI Chat Completion API. The data was generated by running a simple prompt through the model with the `logprobs` option enabled. The HTML file is a minimal placeholder used when experimenting with visualising the probabilities.

## How the JSON was produced

The file `logprobs.json` is the exact response returned by the API call. A prompt was sent to `gpt-4.1-2025-04-14` and the `logprobs` parameter requested per-token probabilities. The model replied with "Hi! How can I help you today?" and the API returned additional metadata, such as usage information, tool selection and sampling parameters (temperature, top\_p etc.).

## Structure of `logprobs.json`

Top level keys include:

- `object`, `id`, `model` – identify the request and model.
- `created` – Unix epoch timestamp when the response was generated.
- `request_id` – unique identifier for the request.
- `usage` – token counts for prompt and completion.
- Sampling parameters: `top_p`, `temperature`, `presence_penalty` and `frequency_penalty`.
- `seed` – RNG seed used by the model.
- `choices` – array of completion choices. Each choice contains the assistant message and its `logprobs`.

The `logprobs` object holds a `content` array. Items inside represent each token of the completion in order. Every token entry contains:

- `token` – the text of the token.
- `logprob` – natural log probability of that token being chosen.
- `bytes` – UTF‑8 byte values for the token.
- `top_logprobs` – list of alternative candidate tokens and their log probabilities for that position.

Example tokens from the stored response:

```json
["Hi", "!", " How", " can", " I", " help", " you", " today", "?"]
```

## Interpreting log probabilities

A higher log probability (closer to zero) means the model considered the token more likely in that context. Values are natural logarithms, so a difference of ~0.69 corresponds to a factor of two in probability. The `top_logprobs` list lets you see which alternatives were considered, ordered by likelihood.

Together, this information can be used to analyse why the model produced the given text and to explore how different sampling parameters affect token selection.
